{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da77bf1-a7a3-4b39-b496-3060adb803bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTraining script for CloudSegSpectralViT.\\nPlace this file in same folder as Vit.py and data_loader.py and run:\\n    python train_ViT_train.py\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_ViT_train.py\n",
    "\"\"\"\n",
    "Training script for CloudSegSpectralViT.\n",
    "Place this file in same folder as Vit.py and data_loader.py and run:\n",
    "    python train_ViT_train.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771aee74-60f8-44da-90f1-3477408f0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206c1cd8-5935-4c19-9f0e-ea3ae8e6d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdd364dd-be3b-412d-a086-b39fcc509361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02890fa0-ca1d-48f1-887a-5a6d62de8667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- adjust these if needed ----\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 8           # change to fit GPU memory\n",
    "NUM_EPOCHS = 2\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NUM_WORKERS = 0\n",
    "PATCH_DIV = 8            # model patch size in Vit.py default (8) -> ensure image dims divisible by this\n",
    "CHECKPOINT_DIR = Path(\"checkpoints\")\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "# --------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9959e0f-2399-4279-8ddc-53e47d44a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import your model (do not modify Vit.py)\n",
    "from Vit import CloudSegSpectralViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f2d6341-bee8-4822-9f4e-a9e5a72cdce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Dataset: reads the dict produced by your data_loader.py\n",
    "# ---------------------------------------------------------\n",
    "class Cloud95Dataset(Dataset):\n",
    "    def __init__(self, samples: List[Dict], transforms=None, crop_size=None):\n",
    "        \"\"\"\n",
    "        samples: list of dicts with keys \"r\",\"g\",\"b\",\"n\",\"m\" where values are Path objects / strings.\n",
    "        transforms: optional callable(image_np, mask_np) -> (image_np, mask_np)\n",
    "        crop_size: optional tuple (H, W) to randomly crop tiles (must be divisible by PATCH_DIV)\n",
    "        \"\"\"\n",
    "        self.samples = samples\n",
    "        self.transforms = transforms\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _open_band(self, p):\n",
    "        if isinstance(p, str):\n",
    "            p = Path(p)\n",
    "        im = Image.open(p)\n",
    "        arr = np.array(im).astype(np.float32)\n",
    "        return arr\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        r = self._open_band(s[\"r\"])\n",
    "        g = self._open_band(s[\"g\"])\n",
    "        b = self._open_band(s[\"b\"])\n",
    "        n = self._open_band(s[\"n\"])\n",
    "        m = self._open_band(s[\"m\"])  # mask\n",
    "\n",
    "        # stack channels -> (H, W, 4)\n",
    "        # If single-channel images are HxW arrays, that's fine.\n",
    "        img = np.stack([r, g, b, n], axis=-1)  # H,W,4\n",
    "\n",
    "        # Normalize to [0,1] per-image (simple)\n",
    "        # To avoid division by zero handle constant images\n",
    "        img = img.astype(np.float32)\n",
    "        # per-band normalization (min-max)\n",
    "        for c in range(img.shape[-1]):\n",
    "            band = img[..., c]\n",
    "            mn, mx = band.min(), band.max()\n",
    "            if mx > mn:\n",
    "                img[..., c] = (band - mn) / (mx - mn)\n",
    "            else:\n",
    "                img[..., c] = band - mn  # zeros\n",
    "\n",
    "        # mask -> ensure single channel and 0/1 labels\n",
    "        if m.ndim == 3:\n",
    "            m = m[..., 0]\n",
    "        mask = (m > 0).astype(np.uint8)  # cloud=1, background=0\n",
    "\n",
    "        H, W, _ = img.shape\n",
    "\n",
    "        # Optional random crop if crop_size provided\n",
    "        if self.crop_size is not None:\n",
    "            ch, cw = self.crop_size\n",
    "            # ensure image is at least crop size\n",
    "            if H < ch or W < cw:\n",
    "                # pad if necessary (simple pad with zeros)\n",
    "                pad_h = max(0, ch - H)\n",
    "                pad_w = max(0, cw - W)\n",
    "                img = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"constant\", constant_values=0)\n",
    "                mask = np.pad(mask, ((0, pad_h), (0, pad_w)), mode=\"constant\", constant_values=0)\n",
    "                H, W = img.shape[:2]\n",
    "\n",
    "            top = random.randint(0, H - ch)\n",
    "            left = random.randint(0, W - cw)\n",
    "            img = img[top:top + ch, left:left + cw]\n",
    "            mask = mask[top:top + ch, left:left + cw]\n",
    "\n",
    "        # final safety: ensure divisibility by PATCH_DIV\n",
    "        H, W = img.shape[:2]\n",
    "        rem_h = H % PATCH_DIV\n",
    "        rem_w = W % PATCH_DIV\n",
    "        if rem_h != 0 or rem_w != 0:\n",
    "            # crop bottom/right to make divisible\n",
    "            H2 = H - rem_h\n",
    "            W2 = W - rem_w\n",
    "            img = img[:H2, :W2]\n",
    "            mask = mask[:H2, :W2]\n",
    "\n",
    "        # transpose to C,H,W and to torch tensors\n",
    "        img = img.transpose(2, 0, 1).astype(np.float32)  # 4,H,W\n",
    "        img_t = torch.from_numpy(img)\n",
    "        mask_t = torch.from_numpy(mask.astype(np.int64))  # for CrossEntropyLoss expect long dtype\n",
    "\n",
    "        return img_t, mask_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daf8677b-63e7-4b2f-a4c3-b5939c598895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# simple IoU metric\n",
    "# ---------------------------------------------------------\n",
    "def batch_iou(pred_logits, target, threshold=0.5, ignore_background=False):\n",
    "    \"\"\"\n",
    "    pred_logits: (B, C, H, W) logits\n",
    "    target: (B, H, W) ints 0..C-1\n",
    "    returns mean IoU over batch for class=1 (cloud) and overall mean\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        probs = torch.softmax(pred_logits, dim=1)\n",
    "        preds = probs.argmax(dim=1)  # (B,H,W)\n",
    "        B = target.shape[0]\n",
    "        ious = []\n",
    "        for b in range(B):\n",
    "            t = target[b].view(-1)\n",
    "            p = preds[b].view(-1)\n",
    "            # class 1 IoU\n",
    "            inter = ((p == 1) & (t == 1)).sum().item()\n",
    "            union = ((p == 1) | (t == 1)).sum().item()\n",
    "            if union == 0:\n",
    "                iou1 = 1.0  # no positive region in both -> perfect for class1\n",
    "            else:\n",
    "                iou1 = inter / union\n",
    "            # overall (mean of class0 & class1)\n",
    "            inter0 = ((p == 0) & (t == 0)).sum().item()\n",
    "            union0 = ((p == 0) | (t == 0)).sum().item()\n",
    "            if union0 == 0:\n",
    "                iou0 = 1.0\n",
    "            else:\n",
    "                iou0 = inter0 / union0\n",
    "            ious.append((iou0 + iou1) / 2.0)\n",
    "        return float(np.mean(ious))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c812e95b-4704-493a-bee1-c90385c19c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded splits -> train: 21041, val: 5260\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# load train/val split produced by data_loader.py\n",
    "# ---------------------------------------------------------\n",
    "SPLIT_PKL = Path(\"train_val_split.pkl\")\n",
    "if not SPLIT_PKL.exists():\n",
    "    raise FileNotFoundError(\"train_val_split.pkl not found. Run data_loader.py first to create it.\")\n",
    "\n",
    "with open(SPLIT_PKL, \"rb\") as f:\n",
    "    splits = pickle.load(f)\n",
    "train_samples = splits[\"train\"]\n",
    "val_samples = splits[\"val\"]\n",
    "\n",
    "print(f\"Loaded splits -> train: {len(train_samples)}, val: {len(val_samples)}\")\n",
    "\n",
    "# Optional: small debug subset\n",
    "# train_samples = train_samples[:200]\n",
    "# val_samples = val_samples[:100]\n",
    "\n",
    "# Create datasets / dataloaders\n",
    "# If your tiles are already target-size (e.g., 256x256), set crop_size=None.\n",
    "# If you want random crops of size 256, set crop_size=(256,256)\n",
    "CROP_SIZE = None  # or (256,256) or (128,128) as per your tile size\n",
    "train_ds = Cloud95Dataset(train_samples, crop_size=CROP_SIZE)\n",
    "val_ds = Cloud95Dataset(val_samples, crop_size=None)  # keep full tiles at val\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40848710-0269-4091-b082-7a8f7d29f8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected input channels: 4\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Build model, loss, optimizer\n",
    "# ---------------------------------------------------------\n",
    "# automatically infer in_ch from dataset first sample\n",
    "sample_img, _ = train_ds[0]\n",
    "in_ch = sample_img.shape[0]\n",
    "print(\"Detected input channels:\", in_ch)\n",
    "\n",
    "model = CloudSegSpectralViT(in_ch=in_ch, num_classes=2, embed_dim=768, depth=8, num_heads=8, patch_size=(8,8), spec_group=3)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # expects logits and target (B,H,W) with values 0..C-1\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "# optional LR scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "# optionally use mixed precision\n",
    "scaler = torch.cuda.amp.GradScaler() if DEVICE.startswith(\"cuda\") else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1a5224-a911-4302-9660-009b6c6bcefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garvi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# training loop\n",
    "# ---------------------------------------------------------\n",
    "best_val_iou = 0.0\n",
    "start_time = time.time()\n",
    "print('begin training')\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    t0 = time.time()\n",
    "    for batch_i, (imgs, masks) in enumerate(train_loader, start=1):\n",
    "        print(f\"\\rbatch {batch_i}\", end='', flush=True)\n",
    "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "        masks = masks.to(DEVICE, non_blocking=True)  # shape (B,H,W)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(imgs)  # (B,2,H,W)\n",
    "                loss = criterion(logits, masks)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_i % 20 == 0:\n",
    "            print(f\"Epoch {epoch} | Batch {batch_i}/{len(train_loader)} | loss: {running_loss / batch_i:.4f}\", end=\"\\r\")\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    t1 = time.time()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_ious = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in val_loader:\n",
    "            imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "            masks = masks.to(DEVICE, non_blocking=True)\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, masks)\n",
    "            val_losses.append(loss.item())\n",
    "            val_ious.append(batch_iou(logits, masks))\n",
    "\n",
    "    mean_val_loss = float(np.mean(val_losses))\n",
    "    mean_val_iou = float(np.mean(val_ious))\n",
    "\n",
    "    print(f\"\\nEpoch {epoch} finished in {(t1-t0):.1f}s | train_loss={epoch_loss:.4f} val_loss={mean_val_loss:.4f} val_iou={mean_val_iou:.4f}\")\n",
    "\n",
    "    # scheduler step (ReduceLROnPlateau)\n",
    "    if scheduler is not None:\n",
    "        scheduler.step(mean_val_iou)\n",
    "\n",
    "    # save checkpoint\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optim_state\": optimizer.state_dict(),\n",
    "        \"val_iou\": mean_val_iou\n",
    "    }\n",
    "    ckpt_path = CHECKPOINT_DIR / f\"ckpt_epoch{epoch:03d}_iou{mean_val_iou:.4f}.pth\"\n",
    "    torch.save(ckpt, ckpt_path)\n",
    "\n",
    "    # keep best\n",
    "    if mean_val_iou > best_val_iou:\n",
    "        best_val_iou = mean_val_iou\n",
    "        best_path = CHECKPOINT_DIR / \"best_model.pth\"\n",
    "        torch.save(ckpt, best_path)\n",
    "        print(f\"  -> New best model saved (val_iou={best_val_iou:.4f})\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Training completed in {total_time/60:.1f} minutes. Best val IoU: {best_val_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0a540-4cc4-43ab-8e78-6460e97da9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
